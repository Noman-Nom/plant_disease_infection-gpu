print("hy")


import os, time, torch
from torch import nn, optim
from torch.utils.data import DataLoader
from torchvision.datasets import FakeData
from torchvision import models, transforms

def main():
    print("Torch:", torch.__version__, "| CUDA build:", torch.version.cuda)
    print("CUDA available:", torch.cuda.is_available())
    if not torch.cuda.is_available():
        raise SystemExit("âŒ CUDA not available in this interpreter. Activate your GPU env and try again.")

    print("GPU(s):", torch.cuda.device_count(), "| Name:", torch.cuda.get_device_name(0))
    torch.backends.cudnn.benchmark = True

    # --- quick CUDA math sanity
    a, b = torch.randn(2048, 2048, device="cuda"), torch.randn(2048, 2048, device="cuda")
    torch.cuda.synchronize(); t0 = time.time()
    c = a @ b
    torch.cuda.synchronize(); t1 = time.time()
    print(f"Matmul OK on {c.device}, shape={tuple(c.shape)}, time={t1-t0:.3f}s")

    # --- tiny fake dataset (no download)
    tfm = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
    ])
    train_ds = FakeData(size=96, image_size=(3, 224, 224), num_classes=10, transform=tmf:=tfm)
    train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)

    # --- lightweight model (resnet18) on GPU
    model = models.resnet18(weights=None, num_classes=10).cuda()
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=3e-4)
    scaler = torch.cuda.amp.GradScaler()

    # one quick epoch (actually just a few batches)
    model.train()
    running_loss = 0.0
    steps = 0
    for images, labels in train_loader:
        images = images.cuda(non_blocking=True)
        labels = labels.cuda(non_blocking=True)

        optimizer.zero_grad(set_to_none=True)
        with torch.cuda.amp.autocast(dtype=torch.float16):
            logits = model(images)
            loss = criterion(logits, labels)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        running_loss += loss.item()
        steps += 1
        if steps >= 5:   # 5 mini-batches are enough for a smoke test
            break

    print(f"Training smoke test âœ…  | steps={steps}, avg_loss={running_loss/steps:.4f}")
    print("All good â€” your GPU env is ready! ðŸš€")

if __name__ == "__main__":
    main()




